import streamlit as st
import asyncio
import tempfile
import os
import uuid
import json
from langchain_core.messages import HumanMessage, AIMessage

# --- ë”¥ë¦¬ì„œì¹˜ ë¹Œë” ì„í¬íŠ¸ ---
# â—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.
from lawdeepresearch.research_agent_full import deep_researcher_builder


# --- LangGraph ê°ì²´ ì´ˆê¸°í™” (ìºì‹±) ---
@st.cache_resource
def get_research_scope():
    from langgraph.checkpoint.memory import InMemorySaver

    checkpointer = InMemorySaver()
    scope = deep_researcher_builder.compile(checkpointer=checkpointer)
    return scope


scope = get_research_scope()

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="ë¶€ë™ì‚° ê³„ì•½ ì•ˆì‹¬ ìŠ¤ìºë„ˆ ğŸ›¡ï¸", layout="wide")
st.title("ë¶€ë™ì‚° ê³„ì•½ ì•ˆì‹¬ ìŠ¤ìºë„ˆ ğŸ›¡ï¸")

# --- 1. ì™¼ìª½ ì‚¬ì´ë“œë°”: íŒŒì¼ ì—…ë¡œë“œ ---
with st.sidebar:
    st.header("íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "ê³„ì•½ì„œ, ë“±ê¸°ë¶€ë“±ë³¸ ë“± ë¶„ì„í•  íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”.",
        type=["pdf", "png", "jpg", "jpeg"],
        accept_multiple_files=True,
    )
    st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•œ í›„, ì¤‘ì•™ í™”ë©´ í•˜ë‹¨ ì±„íŒ…ì°½ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

# --- 2. ì¤‘ì•™ ë©”ì¸ í™”ë©´: ì±„íŒ…ì°½ ---
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "thread_id" not in st.session_state:
    st.session_state.thread_id = str(uuid.uuid4())

# ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if "files" in message and message["files"]:
            st.markdown(f"ğŸ“„ *ì²¨ë¶€ íŒŒì¼: {', '.join(message['files'])}*")
        st.markdown(message["content"])

# í™”ë©´ í•˜ë‹¨ì— ê³ ì •ë˜ëŠ” ì±„íŒ… ì…ë ¥ì°½
if prompt := st.chat_input("íŒŒì¼ì„ ì˜¬ë¦¬ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    attached_file_names = [f.name for f in uploaded_files]
    with st.chat_message("user"):
        if attached_file_names:
            st.markdown(f"ğŸ“„ *ì²¨ë¶€ íŒŒì¼: {', '.join(attached_file_names)}*")
        st.markdown(prompt)

    # ë©”ì‹œì§€ ê¸°ë¡ ì €ì¥
    st.session_state.messages.append(
        {"role": "user", "content": prompt, "files": attached_file_names}
    )

    # AI ì‘ë‹µ ì²˜ë¦¬
    # AI ì‘ë‹µ ì²˜ë¦¬ (ìˆ˜ì •ëœ ì½”ë“œ)
    with st.chat_message("assistant"):
        thinking_placeholder = st.empty()
        answer_placeholder = st.empty()
        temp_dir_obj = None

        try:
            document_paths = []
            if uploaded_files:
                temp_dir_obj = tempfile.TemporaryDirectory()
                for f in uploaded_files:
                    file_path = os.path.join(temp_dir_obj.name, f.name)
                    with open(file_path, "wb") as out_file:
                        out_file.write(f.getbuffer())
                    document_paths.append(file_path)

            async def stream_analysis():
                log_messages = ["ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤..."]
                thinking_placeholder.info("\n\n".join(log_messages))

                config = {"configurable": {"thread_id": st.session_state.thread_id}}
                inputs = {
                    "messages": [HumanMessage(content=prompt)],
                    "document_paths": document_paths,
                }
                final_ai_message = ""

                # ë””ë²„ê¹…ìš© expanderëŠ” ì´ì œ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì‚­ì œí•´ë„ ë©ë‹ˆë‹¤.
                # debug_expander = st.expander("ğŸ•µï¸â€â™‚ï¸ ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ë¡œê·¸ (ë””ë²„ê·¸ìš©)")
                # events_container = debug_expander.container()

                async for event in scope.astream_events(
                    inputs, config=config, version="v2"
                ):
                    # events_container.json(event) # ë””ë²„ê¹… ì™„ë£Œ í›„ ë¹„í™œì„±í™”

                    kind, name = event["event"], event["name"]

                    if kind == "on_chain_start" and name == "process_documents":
                        log_messages.append(
                            "ğŸ“‚ ì²¨ë¶€ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
                        )
                        thinking_placeholder.info("\n\n".join(log_messages))

                    elif kind == "on_tool_start":
                        tool_input = event["data"].get("input", {})
                        query = tool_input.get("query") or json.dumps(
                            tool_input, ensure_ascii=False
                        )

                        if name == "statute_search":
                            log_messages.append(f"âš–ï¸ **ë²•ë ¹ ê²€ìƒ‰:** '{query}'")
                        elif name == "case_law_search":
                            log_messages.append(f"ğŸ‘¨â€âš–ï¸ **íŒë¡€ ê²€ìƒ‰:** '{query}'")
                        elif name == "verify_identity_assumptions":
                            log_messages.append(
                                "ğŸ‘¤ **ëª…ì˜ì í™•ì¸:** ê³„ì•½ì„œì™€ ë“±ê¸°ë¶€ë“±ë³¸ì˜ ëª…ì˜ìë¥¼ ë¹„êµí•©ë‹ˆë‹¤."
                            )
                        elif name == "think_tool":
                            if not log_messages[-1].startswith("ğŸ¤”"):
                                log_messages.append("ğŸ¤” **ë¶„ì„ ê³„íš ìˆ˜ë¦½ ì¤‘...**")
                        elif name == "tavily_search":
                            log_messages.append(f"ğŸŒ **ì›¹ ì •ë³´ ê²€ìƒ‰:** '{query}'")

                        thinking_placeholder.info("\n\n".join(log_messages))

                    # --- ğŸ’¡ [ìˆ˜ì •] 'LangGraph' ì´ë¦„ì˜ ìµœì¢… ì´ë²¤íŠ¸ë¥¼ ì •í™•íˆ í¬ì°© ---
                    elif kind == "on_chain_end" and name == "LangGraph":
                        log_messages.append("âœï¸ í˜„ì¬ê¹Œì§€ì˜ ë¶„ì„ì„ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤ ...")
                        thinking_placeholder.info("\n\n".join(log_messages))

                        final_output_data = event["data"].get("output")

                        if isinstance(final_output_data, dict):
                            # ìš°ì„ ìˆœìœ„ 1: 'final_report' í‚¤ê°€ ì¡´ì¬í•˜ê³  ë‚´ìš©ì´ ìˆë‹¤ë©´, ê·¸ê²ƒì„ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©
                            # log_messages.append(
                            #     "âœ… ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤ ..."
                            # )
                            report_content = final_output_data.get("final_report")
                            if report_content:
                                final_ai_message = report_content

                            # ìš°ì„ ìˆœìœ„ 2: 'final_report'ê°€ ì—†ë‹¤ë©´, ê¸°ì¡´ì²˜ëŸ¼ 'messages' ë¦¬ìŠ¤íŠ¸ì—ì„œ ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ë¥¼ ì°¾ìŒ
                            elif (
                                "messages" in final_output_data
                                and final_output_data["messages"]
                            ):
                                for msg in reversed(final_output_data["messages"]):
                                    if isinstance(msg, AIMessage):
                                        final_ai_message = msg.content
                                        break

                if not final_ai_message:
                    final_ai_message = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

                thinking_placeholder.empty()
                answer_placeholder.markdown(final_ai_message)
                st.session_state.messages.append(
                    {"role": "assistant", "content": final_ai_message}
                )

            asyncio.run(stream_analysis())

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        finally:
            if temp_dir_obj:
                temp_dir_obj.cleanup()
